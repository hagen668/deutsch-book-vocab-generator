{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45983c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib import request\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from pathlib import Path\n",
    "\n",
    "#Import NLTK and Texts\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#Command All Matplotlib Graphs to Appear in Inline in Notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# https://www.gutenberg.org/cache/epub/3600/pg3600-images.html\n",
    "# https://gutendex.com/\n",
    "# https://www.scrapingbee.com/blog/python-web-scraping-beautiful-soup/\n",
    "# https://www.geeksforgeeks.org/find-the-text-of-the-given-tag-using-beautifulsoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# EDIT HERE ##########\n",
    "## EDITING BEGINS\n",
    "book_url = requests.get(\"https://www.gutenberg.org/cache/epub/29336/pg29336-images.html\")\n",
    "file_name_txt = 'may_dict.txt'\n",
    "file_name_csv = 'may_dict.csv'\n",
    "\n",
    "# add additional stop words here as needed\n",
    "stop_words = 'maxime', 'häckerling', 'gretchen', 'margarete', 'mephistopheles', 'wolfgang', 'johann', 'goethe', 'tragödie', 'erster', 'baubo', 'xenien', 'jahre', 'o', 't', 's', 'großes', 'fakultät', 'szene', 'paragraphos', 'lara', 'herminen', 'Weasleyzwilling', 'Wetter', 'namensliste', 'Klassenzimmer', 'wände', 'zahn', 'Zucker', 'Freund', 'Papier', 'Abendessen', 'Pflanze', 'Recht', 'Zeitungsartikel', 'klassenzimmern', 'Fuß', 'Schule', 'Hand', 'Tier', 'künste', 'Nachmittag', 'Tier', 'Kreatur', 'Lehrer', 'Lehrerin', 'Frühstück', 'Porträt', 'Muggelfamilie', 'peev', 'Arbeit', 'leeren', 'Schüler', 'Zombie', 'emmerich', 'Aconitum', 'leg', 'Affodill', 'weasley', 'Potter', 'haben', 'Tages', 'klein', 'bringen', 'Meister', 'erst', 'in', 'vor', 'heißen', 'Tee', 'rasn', 'trinken', 'können', 'ohne', 'sehen', 'spät', 'möchten', 'unfair', 'Paar', 'jetzt', 'Kunst', 'kessel', 'von', 'Poltergeist', 'kommen', 'schick', 'Information', 'Gringott', 'studieren', 'Juli', 'Granger', 'mitkommen', 'manchmal', 'Prinz', 'afrikanisch', 'Tagesprophete', 'quirrells', 'immer', 'nur', 'mit', 'da', 'oft', 'neu', 'gehen', 'argus', 'Filch', 'wo', 'wie', 'ja', 'wenn', 'vielleicht', 'jung', 'alt', 'morgen', 'sehr', 'für', 'klug', 'Freitag', 'freitags', 'hermin', 'Fang', 'Tag', 'Waldes', 'zusammen', 'Mittwoch', 'hier', 'dann', 'denn', 'halb', 'schon', 'allein', 'gut', 'bald', 'lang', 'bis', 'Schuljahresbeginn', 'aber', 'heute', 'Lust', 'Slytherin', 'tags', 'Post', 'Eule', 'gerne', 'Frühstücks', 'kalt', 'warum', 'dich', 'miserabel', 'wild', 'Wild', 'Woche', 'sir', 'extra', 'gern', 'Bruder', 'dort', 'groß', 'Moment', 'und', 'oder', 'Name', 'Professorin', 'Professor', 'Katze', 'Wort', 'Tunnel', 'Tische', 'Thema', 'Wasser', 'Eisenhut', 'Buch', 'Korridor', 'Freitagnachmittag', 'Stapel', 'Teleskope', 'Binn'\n",
    "\n",
    "## EDITING ENDS\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3923dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if book_url.status_code != 200:\n",
    "\tprint(\"Error fetching page\")\n",
    "\texit()\n",
    "else:\n",
    "\tcontent = book_url.content\n",
    "# print(content)\n",
    "\n",
    "soup = BeautifulSoup(book_url.content, 'html.parser')\n",
    "\n",
    "# print(soup.title) # title w HTML\n",
    "# print(soup.title.string) # title as string (no HTML)\n",
    "# print(soup.get_text()) all text (no HTML)\n",
    "\n",
    "data = [element.text for element in soup.find_all(\"p\")]\n",
    "bookstr = ' '.join(data)\n",
    "\n",
    "# data cleaning - tokenize\n",
    "from functions.datacleaning import dataCleaning\n",
    "dc=dataCleaning(bookstr)\n",
    "clean_tokens = dc.textcleaner()\n",
    "\n",
    "typical_gutenberg_stop_words = 'german', 'language', 'may', 'author', 'updated', 'recently', 'most', 'ebook', 'june', 'date', 'release', 'title', 'january', 'february', 'march', 'april', 'july', 'august', 'september', 'october', 'november', 'december'\n",
    "\n",
    "token_list_gutenberg = [w for w in clean_tokens if not w in typical_gutenberg_stop_words]\n",
    "token_list = [w for w in token_list_gutenberg if not w in stop_words]\n",
    "data_dir = os.path.join(\"/Users/thagen/Documents/github-hagen668/deutsch-book-quiz/data/\")\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "# translation prep\n",
    "translator = Translator()\n",
    "flashcards_en = []\n",
    "flashcards_de = []\n",
    "\n",
    "# translation execution\n",
    "for translation in token_list:\n",
    "    result = translator.translate(translation, src='de', dest='en')\n",
    "    flashcards_de.append(result.origin)\n",
    "    flashcards_en.append(result.text)\n",
    "\n",
    "# clean up english translations\n",
    "flashcards_en = [s.replace('to ', '') for s in flashcards_en] \n",
    "\n",
    "# German-English dictionary created here\n",
    "flashcard_dict = dict(zip(flashcards_de, flashcards_en))\n",
    "\n",
    "\n",
    "#### Data exports\n",
    "file_save_txt = os.path.join(data_dir, file_name_txt)\n",
    "\n",
    "### Export dictionary to .txt file\n",
    "# import necessary packages\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "# save as txt\n",
    "with codecs.open(file_save_txt, 'w', encoding='utf-8') as convert_file:\n",
    "     convert_file.write(json.dumps(flashcard_dict, ensure_ascii=False))\n",
    "        \n",
    "        \n",
    "### Export dicitonary to .csv file\n",
    "df_dict = pd.DataFrame(list(flashcard_dict.keys()), columns=['keys'])\n",
    "df_dict['values'] = pd.Series(list(flashcard_dict.values()))\n",
    "\n",
    "file_save_csv = os.path.join(data_dir, file_name_csv)\n",
    "\n",
    "# \"ensure_ascii=False\" persists characters such as umlautes.\n",
    "# save as csv\n",
    "df_dict.to_csv(file_save_csv, sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74977e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### View final word count of work\n",
    "# import collections\n",
    "# od = collections.OrderedDict(sorted(flashcard_dict.items()))\n",
    "# od\n",
    "# len(od)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
